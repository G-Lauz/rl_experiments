{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE: Monte-Carlo Policy-Gradient Control (episodic)\n",
    "From Chapter 13, Policy Gradient Methods (Sutton and Barto, 2018) **[1]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____Observation Space______\n",
      "State space size:  4\n",
      "Sample observation:  [ 2.1195223e+00 -3.2292586e+38  4.1371867e-01 -3.1036727e+38]\n",
      "_____Action Space______\n",
      "Action space size:  2\n",
      "Sample action:  1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"_____Observation Space______\")\n",
    "print(\"State space size: \", state_size)\n",
    "print(\"Sample observation: \", env.observation_space.sample())\n",
    "\n",
    "print(\"_____Action Space______\")\n",
    "print(\"Action space size: \", action_size)\n",
    "print(\"Sample action: \", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Agent (Parametrized Policy)\n",
    "The policy must be differentiable. Here we define a linear policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearPolicyAgent():\n",
    "    def __init__(self, state_size, action_size, alpha=0.0001, gamma=1):\n",
    "        self.alpha = alpha # learning rate\n",
    "        self.gamma = gamma # discount factor\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.weights = torch.rand((action_size, state_size), device=device)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        probabilities = torch.matmul(self.weights, state.T) # compute action preferences eq 13.3\n",
    "        policy = torch.functional.F.softmax(probabilities, dim=0).T # softmax in action preferences eq 13.2\n",
    "\n",
    "        action_idx = torch.multinomial(policy, 1).item() # sample action from policy\n",
    "\n",
    "        return action_idx\n",
    "    \n",
    "    def update_policy(self, weights):\n",
    "        self.weights = weights\n",
    "\n",
    "# test the policy\n",
    "debug_policy = LinearPolicyAgent(state_size, action_size)\n",
    "debug_policy.act(env.reset()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(environment, agent, n_episodes, max_steps):\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        rewards_history = []\n",
    "        action_history = []\n",
    "        state_history = []\n",
    "\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        # Generate an episode S0, A0, R1, ..., ST-1, AT-1, RT, following pi(.|., theta)\n",
    "        for j in range(max_steps):\n",
    "\n",
    "            action_idx = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action_idx)\n",
    "\n",
    "            action_history.append(action_idx)\n",
    "            state_history.append(state)\n",
    "            rewards_history.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_rewards[i] = sum(rewards_history)\n",
    "\n",
    "        # Update policy parameters, eq. 13.8\n",
    "        for t in range(len(state_history))[::-1]: # exclude the goal state from the update\n",
    "            state = torch.from_numpy(state_history[t]).float().unsqueeze(0).to(device)\n",
    "            action_idx = action_history[t]\n",
    "\n",
    "            # compute the return following time t\n",
    "            gt = agent.gamma ** np.arange(len(rewards_history[t:])) * np.array(rewards_history[t:])\n",
    "            gt = agent.gamma ** t * np.sum(gt)\n",
    "\n",
    "            # compute the gradient of the log policy\n",
    "            probabilities = torch.matmul(agent.weights, state.T) # compute action preferences eq 13.3\n",
    "            policy = torch.functional.F.softmax(probabilities, dim=0) # softmax in action preferences eq 13.2\n",
    "            current_features = state[:, action_idx]\n",
    "            grad_log_pi = current_features - torch.dot(policy[action_idx], current_features)\n",
    "\n",
    "            # update the policy parameters\n",
    "            agent.update_policy(agent.weights + agent.alpha * agent.gamma ** t * gt * grad_log_pi)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over 1000 episodes: 34.219\n"
     ]
    }
   ],
   "source": [
    "linear_policy_agent = LinearPolicyAgent(state_size, action_size, alpha=0.01, gamma=1)\n",
    "scores = reinforce(env, linear_policy_agent, n_episodes=1000, max_steps=1000)\n",
    "\n",
    "print(\"Average score over 1000 episodes: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **[1]** R. S. Sutton and A. G. Barto, Reinforcement learning: an introduction, Second edition. in Adaptive computation and machine learning series. Cambridge, Massachusetts: The MIT Press, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
